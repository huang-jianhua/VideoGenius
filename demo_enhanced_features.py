#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
VideoGeniuså¢å¼ºç‰ˆåŠŸèƒ½æ¼”ç¤ºè„šæœ¬
å±•ç¤ºæ™ºèƒ½æ¨¡å‹åˆ‡æ¢ç³»ç»Ÿå’Œä¸“ä¸šçº§è§†é¢‘æ•ˆæœçš„å¼ºå¤§åŠŸèƒ½
"""

import asyncio
import time
import json
from datetime import datetime
from typing import Dict, Any

# å¯¼å…¥å¢å¼ºç‰ˆæœåŠ¡
try:
    from app.services.llm_enhanced import EnhancedLLMService
    from app.services.load_balancer import LoadBalanceStrategy
    from app.services.model_router import ModelStatus
    from app.services.utils import video_effects
    from app.config import config
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨VideoGeniusé¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    exit(1)


def print_banner():
    """æ‰“å°æ¬¢è¿æ¨ªå¹…"""
    print("=" * 80)
    print("ğŸ¬ VideoGenius å¢å¼ºç‰ˆåŠŸèƒ½æ¼”ç¤º")
    print("=" * 80)
    print("ğŸš€ æ™ºèƒ½æ¨¡å‹åˆ‡æ¢ç³»ç»Ÿ")
    print("ğŸ¨ ä¸“ä¸šçº§è§†é¢‘æ•ˆæœç³»ç»Ÿ")
    print("ğŸ“Š å®æ—¶æ€§èƒ½ç›‘æ§")
    print("ğŸ”„ è´Ÿè½½å‡è¡¡ç®¡ç†")
    print("ğŸ§ª A/Bæµ‹è¯•åŠŸèƒ½")
    print("=" * 80)
    print()


def print_section(title: str):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{'='*20} {title} {'='*20}")


def print_status(status: str, message: str):
    """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
    status_icons = {
        'info': 'ğŸ’¡',
        'success': 'âœ…',
        'warning': 'âš ï¸',
        'error': 'âŒ',
        'loading': 'â³'
    }
    icon = status_icons.get(status, 'ğŸ“‹')
    print(f"{icon} {message}")


def demo_video_effects_system():
    """æ¼”ç¤ºä¸“ä¸šçº§è§†é¢‘æ•ˆæœç³»ç»Ÿ"""
    print_section("ä¸“ä¸šçº§è§†é¢‘æ•ˆæœç³»ç»Ÿæ¼”ç¤º")
    
    # è·å–å¯ç”¨çš„è½¬åœºæ•ˆæœ
    print_status('info', "å¯ç”¨çš„è½¬åœºæ•ˆæœ:")
    transitions = video_effects.get_available_transitions()
    for i, transition in enumerate(transitions, 1):
        print(f"  {i:2d}. {transition}")
    
    print()
    
    # è·å–å¯ç”¨çš„æ»¤é•œæ•ˆæœ
    print_status('info', "å¯ç”¨çš„æ»¤é•œæ•ˆæœ:")
    filters = video_effects.get_available_filters()
    for i, filter_name in enumerate(filters, 1):
        print(f"  {i:2d}. {filter_name}")
    
    print()
    
    # è·å–æ•ˆæœé¢„è®¾
    print_status('info', "ä¸“ä¸šæ•ˆæœé¢„è®¾:")
    presets = video_effects.get_available_presets()
    for preset_name, preset_info in presets.items():
        print(f"  â€¢ {preset_info['name']}: {preset_info['description']}")
    
    print()
    
    # æ¼”ç¤ºæ™ºèƒ½æ•ˆæœæ¨è
    print_status('info', "æ™ºèƒ½æ•ˆæœæ¨èæ¼”ç¤º:")
    
    test_subjects = [
        ("äººå·¥æ™ºèƒ½çš„å‘å±•", "tech"),
        ("å¥åº·ç”Ÿæ´»æ–¹å¼", "lifestyle"), 
        ("ä¼ä¸šç®¡ç†ç­–ç•¥", "business"),
        ("è‰ºæœ¯åˆ›ä½œæŠ€å·§", "creative")
    ]
    
    for subject, expected_type in test_subjects:
        from app.services.video import detect_content_type
        detected_type = detect_content_type(subject)
        effects = video_effects.get_recommended_effects(0, 5, detected_type)
        
        print(f"  ä¸»é¢˜: '{subject}'")
        print(f"    æ£€æµ‹ç±»å‹: {detected_type} (é¢„æœŸ: {expected_type})")
        print(f"    æ¨èè½¬åœº: {effects['transition']}")
        print(f"    æ¨èæ»¤é•œ: {effects['filter']}")
        print(f"    å¢å¼ºçº§åˆ«: {effects['enhancement']}")
        print()


def demo_content_type_detection():
    """æ¼”ç¤ºå†…å®¹ç±»å‹æ£€æµ‹"""
    print_section("æ™ºèƒ½å†…å®¹ç±»å‹æ£€æµ‹")
    
    from app.services.video import detect_content_type
    
    test_cases = [
        "Pythonç¼–ç¨‹å…¥é—¨æ•™ç¨‹",
        "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨",
        "å¥åº·é¥®é£Ÿä¸ç”Ÿæ´»æ–¹å¼",
        "ä¼ä¸šæ•°å­—åŒ–è½¬å‹ç­–ç•¥",
        "æ‘„å½±æ„å›¾æŠ€å·§åˆ†äº«",
        "éŸ³ä¹åˆ›ä½œä¸ç¼–æ›²",
        "æ—…è¡Œæ”»ç•¥æ¨è",
        "æŠ•èµ„ç†è´¢åŸºç¡€çŸ¥è¯†"
    ]
    
    print_status('info', "å†…å®¹ç±»å‹æ£€æµ‹ç»“æœ:")
    for subject in test_cases:
        content_type = detect_content_type(subject)
        print(f"  '{subject}' â†’ {content_type}")


def demo_effect_presets():
    """æ¼”ç¤ºæ•ˆæœé¢„è®¾è¯¦æƒ…"""
    print_section("æ•ˆæœé¢„è®¾è¯¦ç»†ä¿¡æ¯")
    
    presets = video_effects.get_available_presets()
    
    for preset_name, preset_info in presets.items():
        print_status('info', f"é¢„è®¾: {preset_info['name']}")
        print(f"  æè¿°: {preset_info['description']}")
        print(f"  æ•ˆæœç»„åˆ:")
        for effect in preset_info['effects']:
            print(f"    â€¢ {effect}")
        print()


async def demo_service_initialization():
    """æ¼”ç¤ºæœåŠ¡åˆå§‹åŒ–"""
    print_section("æœåŠ¡åˆå§‹åŒ–")
    
    print_status('loading', "æ­£åœ¨åˆå§‹åŒ–å¢å¼ºç‰ˆLLMæœåŠ¡...")
    service = EnhancedLLMService()
    
    print_status('success', "å¢å¼ºç‰ˆLLMæœåŠ¡åˆå§‹åŒ–æˆåŠŸï¼")
    
    # é…ç½®æœåŠ¡
    print_status('loading', "é…ç½®æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ...")
    service.configure(
        intelligent_routing=True,
        load_balancing=True,
        failover=True,
        load_balance_strategy=LoadBalanceStrategy.INTELLIGENT
    )
    
    print_status('success', "æ™ºèƒ½è·¯ç”±ç³»ç»Ÿé…ç½®å®Œæˆï¼")
    print_status('info', f"- æ™ºèƒ½è·¯ç”±: {'å¯ç”¨' if service.use_intelligent_routing else 'ç¦ç”¨'}")
    print_status('info', f"- è´Ÿè½½å‡è¡¡: {'å¯ç”¨' if service.use_load_balancing else 'ç¦ç”¨'}")
    print_status('info', f"- æ•…éšœè½¬ç§»: {'å¯ç”¨' if service.use_failover else 'ç¦ç”¨'}")
    
    return service


def demo_service_stats(service: EnhancedLLMService):
    """æ¼”ç¤ºæœåŠ¡ç»Ÿè®¡"""
    print_section("æœåŠ¡ç»Ÿè®¡ä¿¡æ¯")
    
    stats = service.get_service_stats()
    
    print_status('info', f"æ€»è¯·æ±‚æ•°: {stats.get('total_requests', 0)}")
    print_status('info', f"æˆåŠŸç‡: {stats.get('success_rate', 0):.1%}")
    print_status('info', f"å¹³å‡å“åº”æ—¶é—´: {stats.get('avg_response_time', 0):.2f}ç§’")
    print_status('info', f"ç³»ç»Ÿè¿è¡Œæ—¶é—´: {stats.get('uptime_formatted', 'æœªçŸ¥')}")


def demo_model_health_status(service: EnhancedLLMService):
    """æ¼”ç¤ºæ¨¡å‹å¥åº·çŠ¶æ€"""
    print_section("æ¨¡å‹å¥åº·çŠ¶æ€")
    
    health_status = service.get_model_health_status()
    
    if not health_status:
        print_status('warning', "æš‚æ— æ¨¡å‹å¥åº·æ•°æ®ï¼Œæ­£åœ¨è¿›è¡Œå¥åº·æ£€æŸ¥...")
        service.force_health_check()
        health_status = service.get_model_health_status()
    
    # æŒ‰çŠ¶æ€åˆ†ç»„
    healthy_models = []
    degraded_models = []
    unhealthy_models = []
    unknown_models = []
    
    for model_name, metrics in health_status.items():
        if metrics.status == ModelStatus.HEALTHY:
            healthy_models.append(model_name)
        elif metrics.status == ModelStatus.DEGRADED:
            degraded_models.append(model_name)
        elif metrics.status == ModelStatus.UNHEALTHY:
            unhealthy_models.append(model_name)
        else:
            unknown_models.append(model_name)
    
    print_status('success', f"å¥åº·æ¨¡å‹ ({len(healthy_models)}ä¸ª):")
    for model in healthy_models:
        metrics = health_status[model]
        print(f"  â€¢ {model} - å“åº”æ—¶é—´: {metrics.response_time:.2f}s, æˆåŠŸç‡: {metrics.success_rate:.1%}")
    
    if degraded_models:
        print_status('warning', f"é™çº§æ¨¡å‹ ({len(degraded_models)}ä¸ª):")
        for model in degraded_models:
            print(f"  â€¢ {model}")
    
    if unhealthy_models:
        print_status('error', f"ä¸å¥åº·æ¨¡å‹ ({len(unhealthy_models)}ä¸ª):")
        for model in unhealthy_models:
            print(f"  â€¢ {model}")
    
    if unknown_models:
        print_status('info', f"æœªçŸ¥çŠ¶æ€æ¨¡å‹ ({len(unknown_models)}ä¸ª):")
        for model in unknown_models:
            print(f"  â€¢ {model}")


def demo_model_weights(service: EnhancedLLMService):
    """æ¼”ç¤ºæ¨¡å‹æƒé‡é…ç½®"""
    print_section("æ¨¡å‹æƒé‡é…ç½®")
    
    current_weights = service.router.get_model_weights()
    
    print_status('info', "å½“å‰æ¨¡å‹æƒé‡:")
    for model, weight in current_weights.items():
        print(f"  â€¢ {model}: {weight:.2f}")
    
    # æ¼”ç¤ºæƒé‡è°ƒæ•´
    print_status('loading', "æ¼”ç¤ºæƒé‡è‡ªåŠ¨ä¼˜åŒ–...")
    
    # åŸºäºå¥åº·çŠ¶æ€ä¼˜åŒ–æƒé‡
    health_status = service.get_model_health_status()
    new_weights = {}
    
    for model_name, metrics in health_status.items():
        if metrics.status == ModelStatus.HEALTHY:
            # åŸºäºå“åº”æ—¶é—´å’ŒæˆåŠŸç‡è®¡ç®—æƒé‡
            score = (metrics.success_rate * 0.7) + ((10 - min(metrics.response_time, 10)) / 10 * 0.3)
            new_weights[model_name] = max(0.1, score)
        else:
            new_weights[model_name] = 0.1
    
    service.router.update_model_weights(new_weights)
    
    print_status('success', "æƒé‡ä¼˜åŒ–å®Œæˆï¼æ–°æƒé‡:")
    for model, weight in new_weights.items():
        print(f"  â€¢ {model}: {weight:.2f}")


def demo_fallback_chain(service: EnhancedLLMService):
    """æ¼”ç¤ºæ•…éšœè½¬ç§»é“¾"""
    print_section("æ•…éšœè½¬ç§»é“¾é…ç½®")
    
    current_chain = service.router.get_fallback_chain()
    print_status('info', f"å½“å‰æ•…éšœè½¬ç§»é“¾: {' â†’ '.join(current_chain)}")
    
    # åŸºäºå¥åº·çŠ¶æ€ä¼˜åŒ–æ•…éšœè½¬ç§»é“¾
    health_status = service.get_model_health_status()
    
    sorted_models = sorted(
        health_status.items(),
        key=lambda x: (x[1].status == ModelStatus.HEALTHY, x[1].success_rate, -x[1].response_time),
        reverse=True
    )
    
    optimized_chain = [model for model, _ in sorted_models[:6]]
    service.router.set_fallback_chain(optimized_chain)
    
    print_status('success', f"ä¼˜åŒ–åæ•…éšœè½¬ç§»é“¾: {' â†’ '.join(optimized_chain)}")


async def demo_script_generation(service: EnhancedLLMService):
    """æ¼”ç¤ºè„šæœ¬ç”ŸæˆåŠŸèƒ½"""
    print_section("æ™ºèƒ½è„šæœ¬ç”Ÿæˆæ¼”ç¤º")
    
    test_subject = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•"
    
    print_status('loading', f"æ­£åœ¨ç”Ÿæˆä¸»é¢˜ä¸º '{test_subject}' çš„è§†é¢‘è„šæœ¬...")
    
    start_time = time.time()
    
    try:
        script = await service.generate_script_async(test_subject, "", "zh-CN", 2)
        response_time = time.time() - start_time
        
        print_status('success', f"è„šæœ¬ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {response_time:.2f}ç§’")
        print("\nğŸ“ ç”Ÿæˆçš„è„šæœ¬:")
        print("-" * 60)
        print(script)
        print("-" * 60)
        
    except Exception as e:
        print_status('error', f"è„šæœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")


async def demo_terms_generation(service: EnhancedLLMService):
    """æ¼”ç¤ºå…³é”®è¯ç”ŸæˆåŠŸèƒ½"""
    print_section("æ™ºèƒ½å…³é”®è¯ç”Ÿæˆæ¼”ç¤º")
    
    test_subject = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•"
    
    print_status('loading', f"æ­£åœ¨ç”Ÿæˆä¸»é¢˜ä¸º '{test_subject}' çš„å…³é”®è¯...")
    
    start_time = time.time()
    
    try:
        terms = await service.generate_terms_async(test_subject, f"å…³äº{test_subject}çš„è§†é¢‘è„šæœ¬", 5)
        response_time = time.time() - start_time
        
        print_status('success', f"å…³é”®è¯ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {response_time:.2f}ç§’")
        print("\nğŸ·ï¸ ç”Ÿæˆçš„å…³é”®è¯:")
        for i, term in enumerate(terms, 1):
            print(f"  {i}. {term}")
        
    except Exception as e:
        print_status('error', f"å…³é”®è¯ç”Ÿæˆå¤±è´¥: {str(e)}")


def demo_load_balancer_stats(service: EnhancedLLMService):
    """æ¼”ç¤ºè´Ÿè½½å‡è¡¡ç»Ÿè®¡"""
    print_section("è´Ÿè½½å‡è¡¡ç»Ÿè®¡")
    
    load_stats = service.get_load_balancer_stats()
    
    if load_stats:
        print_status('info', "è´Ÿè½½å‡è¡¡ç»Ÿè®¡:")
        for model, stats in load_stats.items():
            if isinstance(stats, dict):
                print(f"  â€¢ {model}:")
                print(f"    - æ´»è·ƒè¯·æ±‚: {stats.get('active_requests', 0)}")
                print(f"    - æ€»è¯·æ±‚: {stats.get('total_requests', 0)}")
                print(f"    - å¹³å‡å“åº”æ—¶é—´: {stats.get('avg_response_time', 0):.2f}s")
    else:
        print_status('warning', "æš‚æ— è´Ÿè½½å‡è¡¡æ•°æ®")


def demo_intelligent_recommendations(service: EnhancedLLMService):
    """æ¼”ç¤ºæ™ºèƒ½æ¨è"""
    print_section("æ™ºèƒ½æ¨èç³»ç»Ÿ")
    
    health_status = service.get_model_health_status()
    
    recommendations = []
    
    # åˆ†æå¥åº·çŠ¶æ€
    healthy_count = sum(1 for metrics in health_status.values() 
                       if metrics.status == ModelStatus.HEALTHY)
    
    if healthy_count < 3:
        recommendations.append("ğŸ”§ å»ºè®®é…ç½®æ›´å¤šAIæ¨¡å‹ä»¥æé«˜ç³»ç»Ÿå¯é æ€§")
    
    # åˆ†æå“åº”æ—¶é—´
    avg_response_times = [metrics.response_time for metrics in health_status.values() 
                         if metrics.response_time > 0]
    if avg_response_times and sum(avg_response_times) / len(avg_response_times) > 5:
        recommendations.append("âš¡ å»ºè®®ä¼˜åŒ–ç½‘ç»œè¿æ¥æˆ–é€‰æ‹©å“åº”æ›´å¿«çš„æ¨¡å‹")
    
    # åˆ†ææˆåŠŸç‡
    low_success_models = [name for name, metrics in health_status.items() 
                         if metrics.success_rate < 0.8 and metrics.total_requests > 0]
    if low_success_models:
        recommendations.append(f"âš ï¸ å»ºè®®æ£€æŸ¥ä»¥ä¸‹æ¨¡å‹é…ç½®: {', '.join(low_success_models)}")
    
    if recommendations:
        print_status('info', "æ™ºèƒ½æ¨è:")
        for rec in recommendations:
            print(f"  {rec}")
    else:
        print_status('success', "ğŸ‰ ç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–ï¼")


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print_banner()
    
    try:
        # 1. ä¸“ä¸šçº§è§†é¢‘æ•ˆæœç³»ç»Ÿæ¼”ç¤º
        demo_video_effects_system()
        
        # 2. å†…å®¹ç±»å‹æ£€æµ‹æ¼”ç¤º
        demo_content_type_detection()
        
        # 3. æ•ˆæœé¢„è®¾æ¼”ç¤º
        demo_effect_presets()
        
        # 4. æœåŠ¡åˆå§‹åŒ–
        service = await demo_service_initialization()
        
        # 5. æœåŠ¡ç»Ÿè®¡
        demo_service_stats(service)
        
        # 6. æ¨¡å‹å¥åº·çŠ¶æ€
        demo_model_health_status(service)
        
        # 7. æ¨¡å‹æƒé‡é…ç½®
        demo_model_weights(service)
        
        # 8. æ•…éšœè½¬ç§»é“¾
        demo_fallback_chain(service)
        
        # 9. è´Ÿè½½å‡è¡¡ç»Ÿè®¡
        demo_load_balancer_stats(service)
        
        # 10. æ™ºèƒ½æ¨è
        demo_intelligent_recommendations(service)
        
        # 11. è„šæœ¬ç”Ÿæˆæ¼”ç¤º
        await demo_script_generation(service)
        
        # 12. å…³é”®è¯ç”Ÿæˆæ¼”ç¤º
        await demo_terms_generation(service)
        
        print_section("æ¼”ç¤ºå®Œæˆ")
        print_status('success', "ğŸ‰ VideoGeniuså¢å¼ºç‰ˆåŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
        print_status('info', "ğŸ’¡ æ‚¨ç°åœ¨å¯ä»¥ä½“éªŒä»¥ä¸‹å¼ºå¤§åŠŸèƒ½:")
        print_status('info', "   ğŸ¬ ä¸“ä¸šçº§è§†é¢‘æ•ˆæœç³»ç»Ÿ - 6ç§é¢„è®¾é£æ ¼")
        print_status('info', "   ğŸ¤– æ™ºèƒ½æ¨¡å‹åˆ‡æ¢ç³»ç»Ÿ - 9ç§AIæ¨¡å‹")
        print_status('info', "   ğŸ“Š å®æ—¶æ€§èƒ½ç›‘æ§ - ä¸“ä¸šçº§å›¾è¡¨")
        print_status('info', "   ğŸ§ª A/Bæµ‹è¯•åŠŸèƒ½ - å¤šæ¨¡å‹å¯¹æ¯”")
        print_status('info', "   ğŸ¨ æ™ºèƒ½æ•ˆæœæ¨è - æ ¹æ®å†…å®¹è‡ªåŠ¨ä¼˜åŒ–")
        print()
        print_status('info', "ğŸš€ å¯åŠ¨æ–¹å¼:")
        print_status('info', "   1. å¯åŠ¨: python -m streamlit run webui/Main.py")
        print_status('info', "   2. è®¿é—®: http://localhost:8501")
        print_status('info', "   3. ä½“éªŒ: ä¸“ä¸šçº§è§†é¢‘æ•ˆæœå’ŒAIæ¨¡å‹ç®¡ç†")
        
    except Exception as e:
        print_status('error', f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        print_status('info', "è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œç½‘ç»œè¿æ¥")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main()) 