#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VideoGenius æ–‡æ¡£è‡ªåŠ¨åŒ–ç»´æŠ¤ç³»ç»Ÿ
æ–‡æ¡£ç›‘æ§å’Œè‡ªåŠ¨æ›´æ–°å·¥å…·

ä½œè€…: AIåŠ©æ‰‹
åˆ›å»ºæ—¶é—´: 2024-12-19
"""

import os
import sys
import json
import datetime
import schedule
import time
import shutil
import re
from pathlib import Path
from typing import List, Dict, Optional
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/doc_automation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CompletedTaskArchiver:
    """å·²å®Œæˆä»»åŠ¡è‡ªåŠ¨å½’æ¡£å™¨"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.archive_dir = self.project_root / "docs" / "å·²å®Œæˆä»»åŠ¡"
        self.completed_patterns = [
            # ä»»åŠ¡è®¡åˆ’æ–‡æ¡£æ¨¡å¼
            r"ä»Šæ—¥å¼€å‘è®¡åˆ’-.*\.md$",
            r"æ˜æ—¥å¼€å‘è®¡åˆ’-.*\.md$", 
            r"ç¬¬\d+ä¸ªç›®æ ‡-.*è¯¦ç»†è§„åˆ’\.md$",
            # å·²å®Œæˆçš„æŠ¥å‘Šæ–‡æ¡£
            r".*é—®é¢˜è§£å†³æŠ¥å‘Š\.md$",
            # å…¶ä»–å¯èƒ½çš„å·²å®Œæˆä»»åŠ¡æ¨¡å¼
            r".*é›†æˆå®ŒæˆæŠ¥å‘Š\.md$",
            r".*å¼€å‘æ€»ç»“\.md$"
        ]
        
        # åˆ›å»ºå½’æ¡£ç›®å½•
        self.archive_dir.mkdir(exist_ok=True)
    
    def identify_completed_tasks(self) -> List[Path]:
        """è¯†åˆ«å·²å®Œæˆçš„ä»»åŠ¡æ–‡æ¡£"""
        logger.info("ğŸ” è¯†åˆ«å·²å®Œæˆçš„ä»»åŠ¡æ–‡æ¡£...")
        
        completed_docs = []
        
        # æœç´¢æ ¹ç›®å½•ä¸‹çš„ä»»åŠ¡æ–‡æ¡£
        for file_path in self.project_root.glob("*.md"):
            if self._is_task_document(file_path):
                if self._is_task_completed(file_path):
                    completed_docs.append(file_path)
                    logger.info(f"âœ… å‘ç°å·²å®Œæˆä»»åŠ¡: {file_path.name}")
        
        logger.info(f"ğŸ¯ å…±è¯†åˆ«å‡º {len(completed_docs)} ä¸ªå·²å®Œæˆä»»åŠ¡æ–‡æ¡£")
        return completed_docs
    
    def _is_task_document(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä»»åŠ¡æ–‡æ¡£"""
        for pattern in self.completed_patterns:
            if re.match(pattern, file_path.name):
                return True
        return False
    
    def _is_task_completed(self, file_path: Path) -> bool:
        """åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æ£€æŸ¥å®ŒæˆçŠ¶æ€æ ‡è¯†
            completion_indicators = [
                "çŠ¶æ€: âœ… å·²å®Œæˆ",
                "âœ… å·²å®Œæˆ",
                "100%å®Œæˆ",
                "åœ†æ»¡å®Œæˆ",
                "âœ… å®Œå…¨è§£å†³",
                "ä»»åŠ¡å·²100%å®Œæˆ",
                "ğŸ‰ ä»Šæ—¥ä»»åŠ¡åœ†æ»¡å®Œæˆ"
            ]
            
            for indicator in completion_indicators:
                if indicator in content:
                    return True
            
            # æ£€æŸ¥è¿›åº¦ç™¾åˆ†æ¯”ï¼ˆ90%ä»¥ä¸Šè®¤ä¸ºåŸºæœ¬å®Œæˆï¼‰
            progress_patterns = [
                r"è¿›åº¦.*?(\d+)%",
                r"å®Œæˆ.*?(\d+)%", 
                r"(\d+)%.*å®Œæˆ"
            ]
            
            for pattern in progress_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    if int(match) >= 90:
                        return True
            
            return False
            
        except Exception as e:
            logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False
    
    def archive_completed_tasks(self, dry_run: bool = False) -> Dict[str, List[str]]:
        """å½’æ¡£å·²å®Œæˆçš„ä»»åŠ¡"""
        logger.info("ğŸ“¦ å¼€å§‹å½’æ¡£å·²å®Œæˆä»»åŠ¡...")
        
        completed_docs = self.identify_completed_tasks()
        results = {
            "archived": [],
            "failed": [],
            "skipped": []
        }
        
        for doc_path in completed_docs:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨å½’æ¡£ç›®å½•ä¸­
                if self.archive_dir in doc_path.parents:
                    results["skipped"].append(str(doc_path))
                    logger.info(f"â­ï¸ è·³è¿‡å·²å½’æ¡£æ–‡æ¡£: {doc_path.name}")
                    continue
                
                # ç”Ÿæˆå½’æ¡£æ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³é¿å…å†²çªï¼‰
                timestamp = datetime.datetime.now().strftime("%Y%m%d")
                archive_name = f"{timestamp}_{doc_path.name}"
                archive_path = self.archive_dir / archive_name
                
                # æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                counter = 1
                while archive_path.exists():
                    archive_name = f"{timestamp}_{counter:02d}_{doc_path.name}"
                    archive_path = self.archive_dir / archive_name
                    counter += 1
                
                if dry_run:
                    logger.info(f"ğŸ”„ [æ¨¡æ‹Ÿ] å°†å½’æ¡£: {doc_path.name} -> {archive_name}")
                    results["archived"].append(f"{doc_path.name} -> {archive_name}")
                else:
                    # æ‰§è¡Œå½’æ¡£ï¼ˆç§»åŠ¨æ–‡ä»¶ï¼‰
                    shutil.move(str(doc_path), str(archive_path))
                    logger.info(f"âœ… å·²å½’æ¡£: {doc_path.name} -> {archive_name}")
                    results["archived"].append(f"{doc_path.name} -> {archive_name}")
                
            except Exception as e:
                logger.error(f"âŒ å½’æ¡£å¤±è´¥ {doc_path.name}: {e}")
                results["failed"].append(f"{doc_path.name}: {e}")
        
        # è¾“å‡ºå½’æ¡£æ€»ç»“
        logger.info("ğŸ“Š å½’æ¡£ä»»åŠ¡å®Œæˆæ€»ç»“:")
        logger.info(f"  âœ… æˆåŠŸå½’æ¡£: {len(results['archived'])} ä¸ª")
        logger.info(f"  â­ï¸ è·³è¿‡æ–‡æ¡£: {len(results['skipped'])} ä¸ª")
        logger.info(f"  âŒ å¤±è´¥æ–‡æ¡£: {len(results['failed'])} ä¸ª")
        
        return results
    
    def create_archive_index(self):
        """åˆ›å»ºå½’æ¡£ç´¢å¼•æ–‡ä»¶"""
        logger.info("ğŸ“ åˆ›å»ºå½’æ¡£ç´¢å¼•...")
        
        index_file = self.archive_dir / "README.md"
        archived_files = list(self.archive_dir.glob("*.md"))
        archived_files = [f for f in archived_files if f.name != "README.md"]
        
        # æŒ‰æ—¶é—´æ’åº
        archived_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        content = f"""# å·²å®Œæˆä»»åŠ¡å½’æ¡£

## ğŸ“‹ å½’æ¡£è¯´æ˜

æœ¬ç›®å½•å­˜æ”¾å·²å®Œæˆçš„ä»»åŠ¡æ–‡æ¡£ï¼ŒåŒ…æ‹¬ï¼š
- å¼€å‘è®¡åˆ’æ–‡æ¡£
- é—®é¢˜è§£å†³æŠ¥å‘Š
- é¡¹ç›®æ€»ç»“æ–‡æ¡£

**å½’æ¡£æ—¶é—´**: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**å½’æ¡£æ–‡ä»¶æ•°**: {len(archived_files)} ä¸ª

## ğŸ“‚ å½’æ¡£æ–‡ä»¶åˆ—è¡¨

"""
        
        for file_path in archived_files:
            # è§£ææ–‡ä»¶ä¿¡æ¯
            mod_time = datetime.datetime.fromtimestamp(file_path.stat().st_mtime)
            file_size = file_path.stat().st_size
            
            # æå–åŸå§‹æ–‡ä»¶å
            original_name = file_path.name
            if re.match(r"^\d{8}_\d{2}_", original_name):
                original_name = "_".join(original_name.split("_")[2:])
            elif re.match(r"^\d{8}_", original_name):
                original_name = "_".join(original_name.split("_")[1:])
            
            content += f"""### ğŸ“„ {original_name}

- **å½’æ¡£æ–‡ä»¶**: `{file_path.name}`
- **å½’æ¡£æ—¶é—´**: {mod_time.strftime("%Y-%m-%d %H:%M")}
- **æ–‡ä»¶å¤§å°**: {file_size:,} å­—èŠ‚

"""
        
        content += f"""
## ğŸ”§ ç®¡ç†è¯´æ˜

### å¦‚ä½•æŸ¥çœ‹å½’æ¡£æ–‡æ¡£
ç›´æ¥åœ¨å½“å‰ç›®å½•ä¸­æŸ¥çœ‹å¯¹åº”çš„markdownæ–‡ä»¶å³å¯ã€‚

### å½’æ¡£æ–‡ä»¶å‘½åè§„åˆ™
- æ ¼å¼: `YYYYMMDD_åŸæ–‡ä»¶å.md`
- å¦‚æœ‰é‡å¤: `YYYYMMDD_NN_åŸæ–‡ä»¶å.md`

### æ¸…ç†ç­–ç•¥
- ä¿ç•™æœ€è¿‘6ä¸ªæœˆçš„å½’æ¡£æ–‡ä»¶
- è¶…è¿‡6ä¸ªæœˆçš„æ–‡ä»¶ä¼šåœ¨å­£åº¦æ¸…ç†æ—¶ç§»é™¤

---
*è‡ªåŠ¨ç”Ÿæˆäº {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
        
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"âœ… å½’æ¡£ç´¢å¼•å·²åˆ›å»º: {index_file}")

class DocumentMonitor:
    """æ–‡æ¡£ç›‘æ§å™¨"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.docs_path = self.project_root / "docs"
        self.status_file = self.docs_path / "é¡¹ç›®çŠ¶æ€æ€»è§ˆ.md"
        self.memory_file = self.project_root / "AIåŠ©æ‰‹è®°å¿†å­˜å‚¨.md"
        
        # æ ¸å¿ƒæ–‡æ¡£åˆ—è¡¨ - è¿™äº›æ–‡æ¡£éœ€è¦é‡ç‚¹ç›‘æ§
        self.core_documents = [
            "AIåŠ©æ‰‹è®°å¿†å­˜å‚¨.md",
            "docs/ai_assistant/AIåŠ©æ‰‹æ‰¿è¯ºè¿½è¸ªç³»ç»Ÿ.md",  # ğŸš¨ æ–°å¢ï¼šæ‰¿è¯ºè¿½è¸ªæœ€é«˜ä¼˜å…ˆçº§
            "README.md",
            "VideoGeniuså…¨é¢å‘å±•è®¡åˆ’.md",
            "docs/ç®¡ç†è§„èŒƒ/é¡¹ç›®çŠ¶æ€æ€»è§ˆ.md",
            "docs/user/å¯åŠ¨è¯´æ˜.md",
            "docs/user/æ™ºèƒ½å¯åŠ¨å·¥å…·ä½¿ç”¨è¯´æ˜.md",
            "docs/ai_assistant/è®°å¿†æ¢å¤æŒ‡å—.md"
        ]
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._ensure_directories()
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        dirs = ["logs", "backups", "backups/ai_memory", "backups/daily_status"]
        for dir_name in dirs:
            (self.project_root / dir_name).mkdir(exist_ok=True)
    
    def check_document_freshness(self) -> Dict[str, Dict]:
        """æ£€æŸ¥æ–‡æ¡£æ–°é²œåº¦"""
        logger.info("å¼€å§‹æ£€æŸ¥æ–‡æ¡£æ–°é²œåº¦...")
        
        freshness_report = {}
        current_time = datetime.datetime.now()
        
        for doc_path in self.core_documents:
            full_path = self.project_root / doc_path
            
            if not full_path.exists():
                freshness_report[doc_path] = {
                    "status": "missing",
                    "message": "æ–‡æ¡£ä¸å­˜åœ¨"
                }
                continue
            
            # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            mod_time = datetime.datetime.fromtimestamp(full_path.stat().st_mtime)
            age_hours = (current_time - mod_time).total_seconds() / 3600
            
            # åˆ¤æ–­æ–‡æ¡£çŠ¶æ€
            if age_hours > 168:  # 7å¤©
                status = "outdated"
                message = f"æ–‡æ¡£å·²è¿‡æœŸ {age_hours:.1f} å°æ—¶"
            elif age_hours > 24:  # 1å¤©
                status = "aging"
                message = f"æ–‡æ¡£éœ€è¦æ›´æ–° {age_hours:.1f} å°æ—¶"
            else:
                status = "fresh"
                message = f"æ–‡æ¡£çŠ¶æ€è‰¯å¥½ {age_hours:.1f} å°æ—¶å‰æ›´æ–°"
            
            freshness_report[doc_path] = {
                "status": status,
                "age_hours": age_hours,
                "last_modified": mod_time.strftime("%Y-%m-%d %H:%M:%S"),
                "message": message
            }
        
        logger.info(f"æ–‡æ¡£æ–°é²œåº¦æ£€æŸ¥å®Œæˆï¼Œå…±æ£€æŸ¥ {len(self.core_documents)} ä¸ªæ–‡æ¡£")
        return freshness_report
    
    def generate_daily_report(self) -> str:
        """ç”Ÿæˆæ¯æ—¥æ–‡æ¡£çŠ¶æ€æŠ¥å‘Š"""
        logger.info("ç”Ÿæˆæ¯æ—¥æ–‡æ¡£çŠ¶æ€æŠ¥å‘Š...")
        
        freshness_report = self.check_document_freshness()
        current_time = datetime.datetime.now()
        
        report = f"""# VideoGenius æ–‡æ¡£çŠ¶æ€æ—¥æŠ¥

**ç”Ÿæˆæ—¶é—´**: {current_time.strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ“Š æ–‡æ¡£çŠ¶æ€æ¦‚è§ˆ

"""
        
        # ç»Ÿè®¡å„çŠ¶æ€æ–‡æ¡£æ•°é‡
        status_counts = {"fresh": 0, "aging": 0, "outdated": 0, "missing": 0}
        for doc_info in freshness_report.values():
            status_counts[doc_info["status"]] += 1
        
        report += f"""- âœ… çŠ¶æ€è‰¯å¥½: {status_counts['fresh']} ä¸ª
- âš ï¸ éœ€è¦æ›´æ–°: {status_counts['aging']} ä¸ª  
- ğŸ”´ å·²è¿‡æœŸ: {status_counts['outdated']} ä¸ª
- âŒ ç¼ºå¤±: {status_counts['missing']} ä¸ª

## ğŸ“‹ è¯¦ç»†çŠ¶æ€

"""
        
        # è¯¦ç»†çŠ¶æ€åˆ—è¡¨
        for doc_path, info in freshness_report.items():
            status_emoji = {
                "fresh": "âœ…",
                "aging": "âš ï¸", 
                "outdated": "ğŸ”´",
                "missing": "âŒ"
            }
            
            report += f"### {status_emoji[info['status']]} {doc_path}\n"
            report += f"- **çŠ¶æ€**: {info['message']}\n"
            if info['status'] != 'missing':
                report += f"- **æœ€åæ›´æ–°**: {info['last_modified']}\n"
            report += "\n"
        
        # å»ºè®®è¡ŒåŠ¨
        report += "## ğŸ¯ å»ºè®®è¡ŒåŠ¨\n\n"
        
        if status_counts['missing'] > 0:
            report += "- ğŸš¨ **ç´§æ€¥**: åˆ›å»ºç¼ºå¤±çš„æ–‡æ¡£\n"
        
        if status_counts['outdated'] > 0:
            report += "- ğŸ”´ **é«˜ä¼˜å…ˆçº§**: æ›´æ–°è¿‡æœŸæ–‡æ¡£\n"
            
        if status_counts['aging'] > 0:
            report += "- âš ï¸ **ä¸­ä¼˜å…ˆçº§**: æ›´æ–°è€åŒ–æ–‡æ¡£\n"
        
        if status_counts['fresh'] == len(freshness_report):
            report += "- ğŸ‰ **çŠ¶æ€è‰¯å¥½**: æ‰€æœ‰æ–‡æ¡£éƒ½æ˜¯æœ€æ–°çš„ï¼\n"
        
        report += f"\n---\n**æŠ¥å‘Šç”Ÿæˆè€…**: AIåŠ©æ‰‹è‡ªåŠ¨åŒ–ç³»ç»Ÿ\n"
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.project_root / "logs" / f"daily_report_{current_time.strftime('%Y%m%d')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"æ¯æ—¥æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return report

class ProjectStatusUpdater:
    """é¡¹ç›®çŠ¶æ€è‡ªåŠ¨æ›´æ–°å™¨"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.status_file = self.project_root / "docs" / "é¡¹ç›®çŠ¶æ€æ€»è§ˆ.md"
    
    def update_daily_status(self):
        """æ¯æ—¥è‡ªåŠ¨æ›´æ–°é¡¹ç›®çŠ¶æ€"""
        logger.info("å¼€å§‹æ›´æ–°é¡¹ç›®çŠ¶æ€...")
        
        current_time = datetime.datetime.now()
        
        try:
            # è¯»å–å½“å‰çŠ¶æ€æ–‡ä»¶
            if self.status_file.exists():
                with open(self.status_file, 'r', encoding='utf-8') as f:
                    content = f.read()
            else:
                logger.warning("é¡¹ç›®çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                content = ""
            
            # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
            updated_content = self._update_timestamp(content, current_time)
            
            # å†™å›æ–‡ä»¶
            with open(self.status_file, 'w', encoding='utf-8') as f:
                f.write(updated_content)
            
            logger.info("é¡¹ç›®çŠ¶æ€æ›´æ–°å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ›´æ–°é¡¹ç›®çŠ¶æ€æ—¶å‡ºé”™: {e}")
    
    def _update_timestamp(self, content: str, timestamp: datetime.datetime) -> str:
        """æ›´æ–°æ–‡æ¡£ä¸­çš„æ—¶é—´æˆ³"""
        lines = content.split('\n')
        updated_lines = []
        
        for line in lines:
            if line.startswith('**æœ€åæ›´æ–°æ—¶é—´**:'):
                updated_lines.append(f'**æœ€åæ›´æ–°æ—¶é—´**: {timestamp.strftime("%Y-%m-%d %H:%M")}')
            elif line.startswith('**ä¸‹æ¬¡æ›´æ–°**:'):
                next_update = timestamp + datetime.timedelta(days=1)
                updated_lines.append(f'**ä¸‹æ¬¡æ›´æ–°**: {next_update.strftime("%Y-%m-%d")}')
            else:
                updated_lines.append(line)
        
        return '\n'.join(updated_lines)

class MemoryBackupSystem:
    """AIè®°å¿†å¤‡ä»½ç³»ç»Ÿ"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.memory_file = self.project_root / "AIåŠ©æ‰‹è®°å¿†å­˜å‚¨.md"
        self.backup_dir = self.project_root / "backups" / "ai_memory"
    
    def backup_ai_memory(self):
        """è‡ªåŠ¨å¤‡ä»½AIåŠ©æ‰‹è®°å¿†"""
        logger.info("å¼€å§‹å¤‡ä»½AIåŠ©æ‰‹è®°å¿†...")
        
        if not self.memory_file.exists():
            logger.warning("AIåŠ©æ‰‹è®°å¿†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½")
            return
        
        try:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.backup_dir / f"memory_backup_{timestamp}.md"
            
            # å¤åˆ¶æ–‡ä»¶
            with open(self.memory_file, 'r', encoding='utf-8') as src:
                content = src.read()
            
            with open(backup_file, 'w', encoding='utf-8') as dst:
                dst.write(content)
            
            logger.info(f"AIè®°å¿†å¤‡ä»½å®Œæˆ: {backup_file}")
            
            # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘7å¤©ï¼‰
            self._cleanup_old_backups()
            
        except Exception as e:
            logger.error(f"å¤‡ä»½AIè®°å¿†æ—¶å‡ºé”™: {e}")
    
    def _cleanup_old_backups(self):
        """æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶"""
        cutoff_time = datetime.datetime.now() - datetime.timedelta(days=7)
        
        for backup_file in self.backup_dir.glob("memory_backup_*.md"):
            if backup_file.stat().st_mtime < cutoff_time.timestamp():
                backup_file.unlink()
                logger.info(f"åˆ é™¤æ—§å¤‡ä»½: {backup_file}")

class DocumentationAutomation:
    """æ–‡æ¡£è‡ªåŠ¨åŒ–ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = project_root
        self.monitor = DocumentMonitor(project_root)
        self.status_updater = ProjectStatusUpdater(project_root)
        self.backup_system = MemoryBackupSystem(project_root)
        self.archiver = CompletedTaskArchiver(project_root)
    
    def run_daily_tasks(self):
        """è¿è¡Œæ¯æ—¥ä»»åŠ¡"""
        logger.info("ğŸŒ… å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–‡æ¡£ç»´æŠ¤ä»»åŠ¡...")
        
        try:
            # 1. ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š
            self.monitor.generate_daily_report()
            
            # 2. æ›´æ–°é¡¹ç›®çŠ¶æ€
            self.status_updater.update_daily_status()
            
            # 3. å¤‡ä»½AIè®°å¿†
            self.backup_system.backup_ai_memory()
            
            # 4. å½’æ¡£å·²å®Œæˆä»»åŠ¡
            self.archiver.archive_completed_tasks()
            self.archiver.create_archive_index()
            
            logger.info("âœ… æ¯æ—¥æ–‡æ¡£ç»´æŠ¤ä»»åŠ¡å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¯æ—¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    
    def run_weekly_tasks(self):
        """è¿è¡Œæ¯å‘¨ä»»åŠ¡"""
        logger.info("ğŸ“… å¼€å§‹æ‰§è¡Œæ¯å‘¨æ–‡æ¡£ç»´æŠ¤ä»»åŠ¡...")
        
        try:
            # 1. æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥
            self._check_consistency()
            
            # 2. æ¸…ç†è¿‡æœŸæ–‡æ¡£
            self._cleanup_expired_docs()
            
            # 3. å¼ºåˆ¶æ‰§è¡Œå½’æ¡£æ£€æŸ¥
            logger.info("ğŸ”„ æ‰§è¡Œæ¯å‘¨å½’æ¡£æ£€æŸ¥...")
            self.archiver.archive_completed_tasks()
            self.archiver.create_archive_index()
            
            logger.info("âœ… æ¯å‘¨æ–‡æ¡£ç»´æŠ¤ä»»åŠ¡å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¯å‘¨ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    
    def _check_consistency(self):
        """æ£€æŸ¥æ–‡æ¡£ä¸€è‡´æ€§"""
        logger.info("æ£€æŸ¥æ–‡æ¡£ä¸€è‡´æ€§...")
        # TODO: å®ç°æ–‡æ¡£ä¸€è‡´æ€§æ£€æŸ¥é€»è¾‘
        pass
    
    def _cleanup_expired_docs(self):
        """æ¸…ç†è¿‡æœŸæ–‡æ¡£"""
        logger.info("æ¸…ç†è¿‡æœŸæ–‡æ¡£...")
        # TODO: å®ç°è¿‡æœŸæ–‡æ¡£æ¸…ç†é€»è¾‘
        pass
    
    def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
        logger.info("ğŸš€ å¯åŠ¨æ–‡æ¡£è‡ªåŠ¨åŒ–ç»´æŠ¤ç³»ç»Ÿ...")
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every().day.at("09:00").do(self.run_daily_tasks)
        schedule.every().monday.at("09:00").do(self.run_weekly_tasks)
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¯æ—¥ä»»åŠ¡
        self.run_daily_tasks()
        
        logger.info("â° å®šæ—¶ä»»åŠ¡å·²è®¾ç½®:")
        logger.info("  - æ¯æ—¥ä»»åŠ¡: 09:00")
        logger.info("  - æ¯å‘¨ä»»åŠ¡: å‘¨ä¸€ 09:00")
        
        # è¿è¡Œè°ƒåº¦å™¨
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ VideoGenius æ–‡æ¡£è‡ªåŠ¨åŒ–ç»´æŠ¤ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºè‡ªåŠ¨åŒ–ç³»ç»Ÿå®ä¾‹
    automation = DocumentationAutomation()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "daily":
            automation.run_daily_tasks()
        elif command == "weekly":
            automation.run_weekly_tasks()
        elif command == "report":
            report = automation.monitor.generate_daily_report()
            print(report)
        elif command == "archive":
            # æ–°å¢å½’æ¡£å‘½ä»¤
            dry_run = "--dry-run" in sys.argv
            results = automation.archiver.archive_completed_tasks(dry_run=dry_run)
            automation.archiver.create_archive_index()
            if dry_run:
                print("ğŸ”„ æ¨¡æ‹Ÿå½’æ¡£å®Œæˆï¼Œä½¿ç”¨ 'python doc_monitor.py archive' æ‰§è¡Œå®é™…å½’æ¡£")
        elif command == "start":
            automation.start_scheduler()
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {command}")
            print("å¯ç”¨å‘½ä»¤: daily, weekly, report, archive [--dry-run], start")
    else:
        # é»˜è®¤å¯åŠ¨è°ƒåº¦å™¨
        automation.start_scheduler()

if __name__ == "__main__":
    main() 