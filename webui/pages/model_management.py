#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AIæ¨¡å‹ç®¡ç†é¡µé¢
æä¾›æ™ºèƒ½æ¨¡å‹åˆ‡æ¢ç³»ç»Ÿçš„Webç•Œé¢ç®¡ç†
"""

import streamlit as st
import asyncio
import time
import json
from typing import Dict, Any, List
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
from datetime import datetime, timedelta

from app.services.llm_enhanced import EnhancedLLMService
from app.services.load_balancer import LoadBalanceStrategy
from app.services.model_router import ModelStatus
from app.config import config


def render_model_management_page():
    """æ¸²æŸ“AIæ¨¡å‹ç®¡ç†é¡µé¢"""
    st.title("ğŸ¤– AIæ¨¡å‹æ™ºèƒ½ç®¡ç†ä¸­å¿ƒ")
    st.markdown("---")
    
    # åˆå§‹åŒ–å¢å¼ºç‰ˆLLMæœåŠ¡
    if 'enhanced_llm_service' not in st.session_state:
        st.session_state.enhanced_llm_service = EnhancedLLMService()
    
    service = st.session_state.enhanced_llm_service
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ¯ æ™ºèƒ½æ§åˆ¶å°", 
        "ğŸ“Š æ¨¡å‹ç›‘æ§", 
        "âš™ï¸ ç³»ç»Ÿé…ç½®", 
        "ğŸ”„ è´Ÿè½½å‡è¡¡", 
        "ğŸ§ª æµ‹è¯•ä¸­å¿ƒ"
    ])
    
    with tab1:
        render_intelligent_dashboard(service)
    
    with tab2:
        render_model_monitoring(service)
    
    with tab3:
        render_system_configuration(service)
    
    with tab4:
        render_load_balancing(service)
    
    with tab5:
        render_testing_center(service)


def render_intelligent_dashboard(service: EnhancedLLMService):
    """æ¸²æŸ“æ™ºèƒ½æ§åˆ¶å°"""
    st.header("ğŸ¯ æ™ºèƒ½æ§åˆ¶å°")
    
    # ç³»ç»ŸçŠ¶æ€æ¦‚è§ˆ
    col1, col2, col3, col4 = st.columns(4)
    
    # è·å–æœåŠ¡ç»Ÿè®¡
    stats = service.get_service_stats()
    health_status = service.get_model_health_status()
    
    with col1:
        st.metric(
            "æ€»è¯·æ±‚æ•°", 
            stats.get('total_requests', 0),
            delta=f"æˆåŠŸç‡ {stats.get('success_rate', 0):.1%}"
        )
    
    with col2:
        healthy_models = sum(1 for metrics in health_status.values() 
                           if metrics.status == ModelStatus.HEALTHY)
        total_models = len(health_status)
        st.metric(
            "å¥åº·æ¨¡å‹", 
            f"{healthy_models}/{total_models}",
            delta=f"{healthy_models/max(1,total_models):.1%} å¯ç”¨"
        )
    
    with col3:
        avg_response = stats.get('avg_response_time', 0)
        st.metric(
            "å¹³å‡å“åº”æ—¶é—´", 
            f"{avg_response:.2f}s",
            delta="ä¼˜ç§€" if avg_response < 5 else "éœ€ä¼˜åŒ–"
        )
    
    with col4:
        uptime = stats.get('uptime_formatted', '0h 0m 0s')
        st.metric(
            "ç³»ç»Ÿè¿è¡Œæ—¶é—´", 
            uptime,
            delta="ç¨³å®šè¿è¡Œ"
        )
    
    st.markdown("---")
    
    # å¿«é€Ÿæ“ä½œé¢æ¿
    st.subheader("ğŸš€ å¿«é€Ÿæ“ä½œ")
    
    col_op1, col_op2, col_op3, col_op4 = st.columns(4)
    
    with col_op1:
        if st.button("ğŸ”„ å¼ºåˆ¶å¥åº·æ£€æŸ¥", use_container_width=True):
            with st.spinner("æ­£åœ¨æ£€æŸ¥æ‰€æœ‰æ¨¡å‹å¥åº·çŠ¶æ€..."):
                service.force_health_check()
                st.success("å¥åº·æ£€æŸ¥å®Œæˆï¼")
                st.rerun()
    
    with col_op2:
        if st.button("ğŸ“Š é‡ç½®ç»Ÿè®¡æ•°æ®", use_container_width=True):
            service.reset_stats()
            st.success("ç»Ÿè®¡æ•°æ®å·²é‡ç½®ï¼")
            st.rerun()
    
    with col_op3:
        if st.button("âš¡ ä¼˜åŒ–é…ç½®", use_container_width=True):
            # è‡ªåŠ¨ä¼˜åŒ–é…ç½®
            optimize_configuration(service)
            st.success("é…ç½®å·²ä¼˜åŒ–ï¼")
            st.rerun()
    
    with col_op4:
        if st.button("ğŸ¯ æ™ºèƒ½æ¨è", use_container_width=True):
            show_intelligent_recommendations(service)
    
    # å®æ—¶æ¨¡å‹çŠ¶æ€
    st.subheader("ğŸ“ˆ å®æ—¶æ¨¡å‹çŠ¶æ€")
    render_real_time_status(health_status)


def render_model_monitoring(service: EnhancedLLMService):
    """æ¸²æŸ“æ¨¡å‹ç›‘æ§é¡µé¢"""
    st.header("ğŸ“Š æ¨¡å‹æ€§èƒ½ç›‘æ§")
    
    # è·å–ç›‘æ§æ•°æ®
    health_status = service.get_model_health_status()
    
    if not health_status:
        st.warning("æš‚æ— ç›‘æ§æ•°æ®ï¼Œè¯·å…ˆè¿›è¡Œå¥åº·æ£€æŸ¥")
        return
    
    # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨
    st.subheader("âš¡ æ€§èƒ½å¯¹æ¯”åˆ†æ")
    
    # å‡†å¤‡æ•°æ®
    model_data = []
    for model_name, metrics in health_status.items():
        model_data.append({
            'Model': model_name,
            'Response Time': metrics.response_time,
            'Success Rate': metrics.success_rate,
            'Total Requests': metrics.total_requests,
            'Status': metrics.status.value,
            'Cost': metrics.cost_per_request
        })
    
    df = pd.DataFrame(model_data)
    
    # å“åº”æ—¶é—´å¯¹æ¯”
    col_chart1, col_chart2 = st.columns(2)
    
    with col_chart1:
        fig_response = px.bar(
            df, 
            x='Model', 
            y='Response Time',
            color='Status',
            title="æ¨¡å‹å“åº”æ—¶é—´å¯¹æ¯”",
            color_discrete_map={
                'healthy': '#00ff00',
                'degraded': '#ffaa00', 
                'unhealthy': '#ff0000',
                'unknown': '#888888'
            }
        )
        fig_response.update_layout(height=400)
        st.plotly_chart(fig_response, use_container_width=True)
    
    with col_chart2:
        fig_success = px.bar(
            df,
            x='Model',
            y='Success Rate', 
            color='Status',
            title="æ¨¡å‹æˆåŠŸç‡å¯¹æ¯”",
            color_discrete_map={
                'healthy': '#00ff00',
                'degraded': '#ffaa00',
                'unhealthy': '#ff0000', 
                'unknown': '#888888'
            }
        )
        fig_success.update_layout(height=400)
        st.plotly_chart(fig_success, use_container_width=True)
    
    # è¯¦ç»†ç›‘æ§è¡¨æ ¼
    st.subheader("ğŸ“‹ è¯¦ç»†ç›‘æ§æ•°æ®")
    
    # æ ¼å¼åŒ–æ•°æ®ç”¨äºæ˜¾ç¤º
    display_data = []
    for model_name, metrics in health_status.items():
        status_emoji = {
            ModelStatus.HEALTHY: "âœ…",
            ModelStatus.DEGRADED: "âš ï¸", 
            ModelStatus.UNHEALTHY: "âŒ",
            ModelStatus.UNKNOWN: "â“"
        }
        
        display_data.append({
            "æ¨¡å‹": model_name,
            "çŠ¶æ€": f"{status_emoji.get(metrics.status, 'â“')} {metrics.status.value}",
            "å“åº”æ—¶é—´": f"{metrics.response_time:.2f}s",
            "æˆåŠŸç‡": f"{metrics.success_rate:.1%}",
            "æ€»è¯·æ±‚": metrics.total_requests,
            "æˆåŠŸè¯·æ±‚": metrics.successful_requests,
            "å¤±è´¥è¯·æ±‚": metrics.failed_requests,
            "è¿ç»­å¤±è´¥": metrics.consecutive_failures,
            "æˆæœ¬/è¯·æ±‚": f"Â¥{metrics.cost_per_request:.4f}",
            "æœ€åæ£€æŸ¥": datetime.fromtimestamp(metrics.last_check_time).strftime("%H:%M:%S") if metrics.last_check_time > 0 else "æœªæ£€æŸ¥"
        })
    
    st.dataframe(display_data, use_container_width=True)


def render_system_configuration(service: EnhancedLLMService):
    """æ¸²æŸ“ç³»ç»Ÿé…ç½®é¡µé¢"""
    st.header("âš™ï¸ æ™ºèƒ½è·¯ç”±ç³»ç»Ÿé…ç½®")
    
    # å½“å‰é…ç½®çŠ¶æ€
    st.subheader("ğŸ“‹ å½“å‰é…ç½®")
    
    col_config1, col_config2 = st.columns(2)
    
    with col_config1:
        st.markdown("**ç³»ç»ŸåŠŸèƒ½çŠ¶æ€**")
        
        # æ™ºèƒ½è·¯ç”±
        intelligent_routing = st.checkbox(
            "ğŸ§  æ™ºèƒ½è·¯ç”±", 
            value=service.use_intelligent_routing,
            help="æ ¹æ®æ¨¡å‹æ€§èƒ½è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹"
        )
        
        # è´Ÿè½½å‡è¡¡
        load_balancing = st.checkbox(
            "âš–ï¸ è´Ÿè½½å‡è¡¡",
            value=service.use_load_balancing, 
            help="åœ¨å¤šä¸ªæ¨¡å‹é—´åˆ†é…è¯·æ±‚è´Ÿè½½"
        )
        
        # æ•…éšœè½¬ç§»
        failover = st.checkbox(
            "ğŸ”„ æ•…éšœè½¬ç§»",
            value=service.use_failover,
            help="æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹"
        )
    
    with col_config2:
        st.markdown("**è´Ÿè½½å‡è¡¡ç­–ç•¥**")
        
        strategy_options = {
            "æ™ºèƒ½é€‰æ‹©": LoadBalanceStrategy.INTELLIGENT,
            "è½®è¯¢": LoadBalanceStrategy.ROUND_ROBIN,
            "åŠ æƒè½®è¯¢": LoadBalanceStrategy.WEIGHTED_ROUND_ROBIN,
            "æœ€å°‘è¿æ¥": LoadBalanceStrategy.LEAST_CONNECTIONS,
            "å“åº”æ—¶é—´ä¼˜å…ˆ": LoadBalanceStrategy.RESPONSE_TIME,
            "éšæœº": LoadBalanceStrategy.RANDOM
        }
        
        selected_strategy = st.selectbox(
            "é€‰æ‹©ç­–ç•¥",
            options=list(strategy_options.keys()),
            index=0,
            help="é€‰æ‹©è´Ÿè½½å‡è¡¡ç­–ç•¥"
        )
        
        strategy = strategy_options[selected_strategy]
    
    # åº”ç”¨é…ç½®
    if st.button("ğŸ’¾ åº”ç”¨é…ç½®", use_container_width=True):
        service.configure(
            intelligent_routing=intelligent_routing,
            load_balancing=load_balancing,
            failover=failover,
            load_balance_strategy=strategy
        )
        st.success("é…ç½®å·²åº”ç”¨ï¼")
        st.rerun()
    
    st.markdown("---")
    
    # æ¨¡å‹æƒé‡é…ç½®
    st.subheader("âš–ï¸ æ¨¡å‹æƒé‡é…ç½®")
    
    current_weights = service.router.get_model_weights()
    
    st.markdown("**è°ƒæ•´æ¨¡å‹æƒé‡ï¼ˆæƒé‡è¶Šé«˜ï¼Œè¢«é€‰ä¸­æ¦‚ç‡è¶Šå¤§ï¼‰**")
    
    new_weights = {}
    weight_cols = st.columns(3)
    
    for i, (model, weight) in enumerate(current_weights.items()):
        col_idx = i % 3
        with weight_cols[col_idx]:
            new_weights[model] = st.slider(
                f"{model}",
                min_value=0.0,
                max_value=2.0,
                value=weight,
                step=0.1,
                help=f"å½“å‰æƒé‡: {weight}"
            )
    
    if st.button("ğŸ”„ æ›´æ–°æƒé‡", use_container_width=True):
        service.router.update_model_weights(new_weights)
        st.success("æ¨¡å‹æƒé‡å·²æ›´æ–°ï¼")
    
    # å¤‡ç”¨é“¾é…ç½®
    st.subheader("ğŸ”— æ•…éšœè½¬ç§»é“¾é…ç½®")
    
    current_chain = service.router.get_fallback_chain()
    available_models = list(current_weights.keys())
    
    st.markdown("**é…ç½®æ•…éšœè½¬ç§»é¡ºåºï¼ˆæ‹–æ‹½æ’åºï¼‰**")
    
    # ä½¿ç”¨å¤šé€‰æ¡†é…ç½®å¤‡ç”¨é“¾
    new_chain = st.multiselect(
        "é€‰æ‹©å¹¶æ’åºå¤‡ç”¨æ¨¡å‹",
        options=available_models,
        default=current_chain,
        help="æŒ‰ä¼˜å…ˆçº§é¡ºåºé€‰æ‹©å¤‡ç”¨æ¨¡å‹"
    )
    
    if st.button("ğŸ”— æ›´æ–°å¤‡ç”¨é“¾", use_container_width=True):
        service.router.set_fallback_chain(new_chain)
        st.success("å¤‡ç”¨é“¾å·²æ›´æ–°ï¼")


def render_load_balancing(service: EnhancedLLMService):
    """æ¸²æŸ“è´Ÿè½½å‡è¡¡é¡µé¢"""
    st.header("ğŸ”„ è´Ÿè½½å‡è¡¡ç®¡ç†")
    
    # è·å–è´Ÿè½½å‡è¡¡ç»Ÿè®¡
    load_stats = service.get_load_balancer_stats()
    
    if not load_stats:
        st.warning("æš‚æ— è´Ÿè½½å‡è¡¡æ•°æ®")
        return
    
    # è´Ÿè½½åˆ†å¸ƒå›¾è¡¨
    st.subheader("ğŸ“Š è´Ÿè½½åˆ†å¸ƒåˆ†æ")
    
    # å‡†å¤‡è´Ÿè½½æ•°æ®
    load_data = []
    for model, stats in load_stats.items():
        if isinstance(stats, dict):
            load_data.append({
                'Model': model,
                'Active Requests': stats.get('active_requests', 0),
                'Total Requests': stats.get('total_requests', 0),
                'Avg Response Time': stats.get('avg_response_time', 0)
            })
    
    if load_data:
        df_load = pd.DataFrame(load_data)
        
        col_load1, col_load2 = st.columns(2)
        
        with col_load1:
            # æ´»è·ƒè¯·æ±‚åˆ†å¸ƒ
            fig_active = px.pie(
                df_load,
                values='Active Requests',
                names='Model',
                title="å½“å‰æ´»è·ƒè¯·æ±‚åˆ†å¸ƒ"
            )
            st.plotly_chart(fig_active, use_container_width=True)
        
        with col_load2:
            # æ€»è¯·æ±‚åˆ†å¸ƒ
            fig_total = px.pie(
                df_load,
                values='Total Requests', 
                names='Model',
                title="æ€»è¯·æ±‚åˆ†å¸ƒ"
            )
            st.plotly_chart(fig_total, use_container_width=True)
        
        # è´Ÿè½½è¯¦æƒ…è¡¨æ ¼
        st.subheader("ğŸ“‹ è´Ÿè½½è¯¦æƒ…")
        st.dataframe(df_load, use_container_width=True)


def render_testing_center(service: EnhancedLLMService):
    """æ¸²æŸ“æµ‹è¯•ä¸­å¿ƒ"""
    st.header("ğŸ§ª AIæ¨¡å‹æµ‹è¯•ä¸­å¿ƒ")
    
    # æµ‹è¯•é…ç½®
    st.subheader("âš™ï¸ æµ‹è¯•é…ç½®")
    
    col_test1, col_test2 = st.columns(2)
    
    with col_test1:
        test_subject = st.text_input(
            "æµ‹è¯•ä¸»é¢˜",
            value="äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿",
            help="è¾“å…¥è¦æµ‹è¯•çš„è§†é¢‘ä¸»é¢˜"
        )
        
        test_language = st.selectbox(
            "è¯­è¨€",
            options=["zh-CN", "en-US"],
            help="é€‰æ‹©ç”Ÿæˆè¯­è¨€"
        )
    
    with col_test2:
        paragraph_number = st.slider(
            "æ®µè½æ•°é‡",
            min_value=1,
            max_value=5,
            value=2,
            help="ç”Ÿæˆçš„æ®µè½æ•°é‡"
        )
        
        terms_amount = st.slider(
            "å…³é”®è¯æ•°é‡", 
            min_value=3,
            max_value=10,
            value=5,
            help="ç”Ÿæˆçš„å…³é”®è¯æ•°é‡"
        )
    
    # æµ‹è¯•æŒ‰é’®
    col_btn1, col_btn2, col_btn3 = st.columns(3)
    
    with col_btn1:
        if st.button("ğŸ“ æµ‹è¯•è„šæœ¬ç”Ÿæˆ", use_container_width=True):
            test_script_generation(service, test_subject, test_language, paragraph_number)
    
    with col_btn2:
        if st.button("ğŸ·ï¸ æµ‹è¯•å…³é”®è¯ç”Ÿæˆ", use_container_width=True):
            test_terms_generation(service, test_subject, terms_amount)
    
    with col_btn3:
        if st.button("ğŸ”„ A/Bå¯¹æ¯”æµ‹è¯•", use_container_width=True):
            test_ab_comparison(service, test_subject, test_language)


def render_real_time_status(health_status: Dict[str, Any]):
    """æ¸²æŸ“å®æ—¶çŠ¶æ€"""
    
    # æŒ‰çŠ¶æ€åˆ†ç»„æ˜¾ç¤º
    healthy_models = []
    degraded_models = []
    unhealthy_models = []
    unknown_models = []
    
    for model_name, metrics in health_status.items():
        if metrics.status == ModelStatus.HEALTHY:
            healthy_models.append(model_name)
        elif metrics.status == ModelStatus.DEGRADED:
            degraded_models.append(model_name)
        elif metrics.status == ModelStatus.UNHEALTHY:
            unhealthy_models.append(model_name)
        else:
            unknown_models.append(model_name)
    
    col_status1, col_status2, col_status3, col_status4 = st.columns(4)
    
    with col_status1:
        st.success(f"âœ… å¥åº·æ¨¡å‹ ({len(healthy_models)})")
        for model in healthy_models:
            st.write(f"â€¢ {model}")
    
    with col_status2:
        if degraded_models:
            st.warning(f"âš ï¸ é™çº§æ¨¡å‹ ({len(degraded_models)})")
            for model in degraded_models:
                st.write(f"â€¢ {model}")
    
    with col_status3:
        if unhealthy_models:
            st.error(f"âŒ ä¸å¥åº·æ¨¡å‹ ({len(unhealthy_models)})")
            for model in unhealthy_models:
                st.write(f"â€¢ {model}")
    
    with col_status4:
        if unknown_models:
            st.info(f"â“ æœªçŸ¥çŠ¶æ€ ({len(unknown_models)})")
            for model in unknown_models:
                st.write(f"â€¢ {model}")


def optimize_configuration(service: EnhancedLLMService):
    """è‡ªåŠ¨ä¼˜åŒ–é…ç½®"""
    # åŸºäºå½“å‰æ€§èƒ½æ•°æ®è‡ªåŠ¨ä¼˜åŒ–é…ç½®
    health_status = service.get_model_health_status()
    
    # è®¡ç®—æœ€ä½³æƒé‡
    new_weights = {}
    for model_name, metrics in health_status.items():
        if metrics.status == ModelStatus.HEALTHY:
            # åŸºäºå“åº”æ—¶é—´å’ŒæˆåŠŸç‡è®¡ç®—æƒé‡
            score = (metrics.success_rate * 0.7) + ((10 - min(metrics.response_time, 10)) / 10 * 0.3)
            new_weights[model_name] = max(0.1, score)
        else:
            new_weights[model_name] = 0.1
    
    # åº”ç”¨ä¼˜åŒ–æƒé‡
    service.router.update_model_weights(new_weights)
    
    # ä¼˜åŒ–å¤‡ç”¨é“¾
    sorted_models = sorted(
        health_status.items(),
        key=lambda x: (x[1].status == ModelStatus.HEALTHY, x[1].success_rate, -x[1].response_time),
        reverse=True
    )
    
    optimized_chain = [model for model, _ in sorted_models[:6]]
    service.router.set_fallback_chain(optimized_chain)


def show_intelligent_recommendations(service: EnhancedLLMService):
    """æ˜¾ç¤ºæ™ºèƒ½æ¨è"""
    health_status = service.get_model_health_status()
    
    recommendations = []
    
    # åˆ†æå¥åº·çŠ¶æ€
    healthy_count = sum(1 for metrics in health_status.values() 
                       if metrics.status == ModelStatus.HEALTHY)
    
    if healthy_count < 3:
        recommendations.append("ğŸ”§ å»ºè®®é…ç½®æ›´å¤šAIæ¨¡å‹ä»¥æé«˜ç³»ç»Ÿå¯é æ€§")
    
    # åˆ†æå“åº”æ—¶é—´
    avg_response_times = [metrics.response_time for metrics in health_status.values() 
                         if metrics.response_time > 0]
    if avg_response_times and sum(avg_response_times) / len(avg_response_times) > 5:
        recommendations.append("âš¡ å»ºè®®ä¼˜åŒ–ç½‘ç»œè¿æ¥æˆ–é€‰æ‹©å“åº”æ›´å¿«çš„æ¨¡å‹")
    
    # åˆ†ææˆåŠŸç‡
    low_success_models = [name for name, metrics in health_status.items() 
                         if metrics.success_rate < 0.8 and metrics.total_requests > 0]
    if low_success_models:
        recommendations.append(f"âš ï¸ å»ºè®®æ£€æŸ¥ä»¥ä¸‹æ¨¡å‹é…ç½®: {', '.join(low_success_models)}")
    
    if recommendations:
        st.info("ğŸ’¡ æ™ºèƒ½æ¨è:\n" + "\n".join(recommendations))
    else:
        st.success("ğŸ‰ ç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–ï¼")


def test_script_generation(service: EnhancedLLMService, subject: str, language: str, paragraphs: int):
    """æµ‹è¯•è„šæœ¬ç”Ÿæˆ"""
    with st.spinner("æ­£åœ¨ç”Ÿæˆè„šæœ¬..."):
        start_time = time.time()
        
        try:
            # ä½¿ç”¨åŒæ­¥æ¥å£è¿›è¡Œæµ‹è¯•
            script = service.generate_script(subject, "", language, paragraphs)
            
            response_time = time.time() - start_time
            
            st.success(f"âœ… è„šæœ¬ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {response_time:.2f}ç§’")
            
            with st.expander("ğŸ“ ç”Ÿæˆçš„è„šæœ¬", expanded=True):
                st.write(script)
                
        except Exception as e:
            st.error(f"âŒ è„šæœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")


def test_terms_generation(service: EnhancedLLMService, subject: str, amount: int):
    """æµ‹è¯•å…³é”®è¯ç”Ÿæˆ"""
    with st.spinner("æ­£åœ¨ç”Ÿæˆå…³é”®è¯..."):
        start_time = time.time()
        
        try:
            # ä½¿ç”¨åŒæ­¥æ¥å£è¿›è¡Œæµ‹è¯•
            terms = service.generate_terms(subject, f"å…³äº{subject}çš„è§†é¢‘è„šæœ¬", amount)
            
            response_time = time.time() - start_time
            
            st.success(f"âœ… å…³é”®è¯ç”ŸæˆæˆåŠŸï¼è€—æ—¶: {response_time:.2f}ç§’")
            
            with st.expander("ğŸ·ï¸ ç”Ÿæˆçš„å…³é”®è¯", expanded=True):
                for i, term in enumerate(terms, 1):
                    st.write(f"{i}. {term}")
                    
        except Exception as e:
            st.error(f"âŒ å…³é”®è¯ç”Ÿæˆå¤±è´¥: {str(e)}")


def test_ab_comparison(service: EnhancedLLMService, subject: str, language: str):
    """A/Bå¯¹æ¯”æµ‹è¯•"""
    st.info("ğŸ”„ æ­£åœ¨è¿›è¡ŒA/Bå¯¹æ¯”æµ‹è¯•ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
    
    # è·å–å¯ç”¨æ¨¡å‹
    health_status = service.get_model_health_status()
    healthy_models = [name for name, metrics in health_status.items() 
                     if metrics.status == ModelStatus.HEALTHY]
    
    if len(healthy_models) < 2:
        st.warning("éœ€è¦è‡³å°‘2ä¸ªå¥åº·çš„æ¨¡å‹æ‰èƒ½è¿›è¡ŒA/Bæµ‹è¯•")
        return
    
    # é€‰æ‹©å‰ä¸¤ä¸ªå¥åº·æ¨¡å‹è¿›è¡Œå¯¹æ¯”
    model_a = healthy_models[0]
    model_b = healthy_models[1]
    
    results = {}
    
    # æµ‹è¯•æ¨¡å‹A
    with st.spinner(f"æ­£åœ¨æµ‹è¯•æ¨¡å‹ {model_a}..."):
        try:
            # ä¸´æ—¶åˆ‡æ¢åˆ°æ¨¡å‹A
            original_provider = config.app.get("llm_provider")
            config.app["llm_provider"] = model_a
            
            start_time = time.time()
            script_a = service.generate_script(subject, "", language, 2)
            time_a = time.time() - start_time
            
            results[model_a] = {
                'script': script_a,
                'time': time_a,
                'success': True
            }
            
        except Exception as e:
            results[model_a] = {
                'script': f"ç”Ÿæˆå¤±è´¥: {str(e)}",
                'time': 0,
                'success': False
            }
        finally:
            # æ¢å¤åŸå§‹é…ç½®
            if 'original_provider' in locals() and original_provider:
                config.app["llm_provider"] = original_provider
    
    # æµ‹è¯•æ¨¡å‹B
    with st.spinner(f"æ­£åœ¨æµ‹è¯•æ¨¡å‹ {model_b}..."):
        try:
            # ä¸´æ—¶åˆ‡æ¢åˆ°æ¨¡å‹B
            original_provider = config.app.get("llm_provider")
            config.app["llm_provider"] = model_b
            
            start_time = time.time()
            script_b = service.generate_script(subject, "", language, 2)
            time_b = time.time() - start_time
            
            results[model_b] = {
                'script': script_b,
                'time': time_b,
                'success': True
            }
            
        except Exception as e:
            results[model_b] = {
                'script': f"ç”Ÿæˆå¤±è´¥: {str(e)}",
                'time': 0,
                'success': False
            }
        finally:
            # æ¢å¤åŸå§‹é…ç½®
            if 'original_provider' in locals() and original_provider:
                config.app["llm_provider"] = original_provider
    
    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    st.subheader("ğŸ“Š A/Bæµ‹è¯•ç»“æœ")
    
    col_a, col_b = st.columns(2)
    
    with col_a:
        st.markdown(f"### æ¨¡å‹ {model_a}")
        if results[model_a]['success']:
            st.success(f"âœ… ç”ŸæˆæˆåŠŸ - è€—æ—¶: {results[model_a]['time']:.2f}ç§’")
            with st.expander("æŸ¥çœ‹è„šæœ¬"):
                st.write(results[model_a]['script'])
        else:
            st.error("âŒ ç”Ÿæˆå¤±è´¥")
            st.write(results[model_a]['script'])
    
    with col_b:
        st.markdown(f"### æ¨¡å‹ {model_b}")
        if results[model_b]['success']:
            st.success(f"âœ… ç”ŸæˆæˆåŠŸ - è€—æ—¶: {results[model_b]['time']:.2f}ç§’")
            with st.expander("æŸ¥çœ‹è„šæœ¬"):
                st.write(results[model_b]['script'])
        else:
            st.error("âŒ ç”Ÿæˆå¤±è´¥")
            st.write(results[model_b]['script'])
    
    # æ€§èƒ½å¯¹æ¯”
    if results[model_a]['success'] and results[model_b]['success']:
        st.subheader("âš¡ æ€§èƒ½å¯¹æ¯”")
        
        comparison_data = pd.DataFrame({
            'Model': [model_a, model_b],
            'Response Time (s)': [results[model_a]['time'], results[model_b]['time']],
            'Script Length': [len(results[model_a]['script']), len(results[model_b]['script'])]
        })
        
        fig_comparison = px.bar(
            comparison_data,
            x='Model',
            y='Response Time (s)',
            title="å“åº”æ—¶é—´å¯¹æ¯”"
        )
        st.plotly_chart(fig_comparison, use_container_width=True)


if __name__ == "__main__":
    render_model_management_page() 